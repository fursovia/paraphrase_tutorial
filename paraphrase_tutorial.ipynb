{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAAI Summer School 2019\n",
    "\n",
    "This tutorial is prepared by Ivan Fursov at Tinkoff.\n",
    "\n",
    "Telegram: [@fursov](https://tele.click/fursov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase identification\n",
    "\n",
    "**Download** files from [here](https://yadi.sk/d/hvxpunMTd2xj2g)\n",
    "\n",
    "Task: given a pair of sentences, classify them as paraphrases or not paraphrases\n",
    "\n",
    "Dataset: [Quora Question Pairs](https://www.kaggle.com/quora/question-pairs-dataset)\n",
    "\n",
    "Quora's first public dataset is related to the problem of identifying duplicate questions. At Quora, an important product principle is that there should be a single question page for each logically distinct question. For example, the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pd.set_option('max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z ]\", \" \", string)  \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/questions.csv', nrows=30000)\n",
    "data = data.dropna()\n",
    "\n",
    "data['question1'] = data['question1'].apply(clean_string)\n",
    "data['question2'] = data['question2'].apply(clean_string)\n",
    "\n",
    "data = data[['question1', 'question2', 'is_duplicate']]\n",
    "data.columns = ['text1', 'text2', 'labels']\n",
    "\n",
    "data = data[data['text1'].apply(lambda x: len(x) > 0) & (data['text2'].apply(lambda x: len(x) > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Dev/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/dev/test -> 70/15/15\n",
    "data_splits = ('train', 'dev', 'test')\n",
    "\n",
    "train, intermediate = train_test_split(data, test_size=0.3, random_state=24)\n",
    "dev, test = train_test_split(intermediate, test_size=0.5, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Text representations\n",
    "### Bag-of-Words\n",
    "\n",
    "Bag of Words (BoW) is an algorithm that counts how many times a word appears in a document. Those word counts allow us to compare documents and gauge their similarities for applications like search, document classification and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\displaystyle {\\text{similarity}}=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_distance(textA, textB):\n",
    "    textA = normalize(textA)\n",
    "    textB = normalize(textB)\n",
    "    if isinstance(textA, np.ndarray):\n",
    "        dot_product = np.multiply(textA, textB).sum(axis=1).flatten()\n",
    "    else:\n",
    "        dot_product = np.array(textA.multiply(textB).sum(axis=1)).flatten()\n",
    "    return 1 - dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit(train['text1'].tolist() + train['text2'].tolist())\n",
    "\n",
    "bow_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    bow_data[name]['text1'] = bow.transform(d['text1'].tolist())\n",
    "    bow_data[name]['text2'] = bow.transform(d['text2'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_data['train']['text1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_data['train']['text2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(textA, textB, labels):\n",
    "    cos_dists = calculate_cosine_distance(textA, textB)\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_thres = None\n",
    "\n",
    "    for thres in np.linspace(0, 2, num=50):\n",
    "        f1 = f1_score((cos_dists < thres).astype(np.int32), labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thres = thres\n",
    "            \n",
    "    return best_f1, best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    bow_data['dev']['text1'], \n",
    "    bow_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_dists = calculate_cosine_distance(bow_data['test']['text1'], bow_data['test']['text2'])\n",
    "\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf\n",
    "\n",
    "Term-frequency-inverse document frequency (TF-IDF) is another way to represent a text by the words it contains. With TF-IDF, words are given weight – TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\n",
    "\n",
    "<img src=\"https://skymind.ai/images/wiki/tfidf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(train['text1'].tolist() + train['text2'].tolist())\n",
    "\n",
    "tfidf_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    tfidf_data[name]['text1'] = tfidf.transform(d['text1'].tolist())\n",
    "    tfidf_data[name]['text2'] = tfidf.transform(d['text2'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data['train']['text1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    tfidf_data['dev']['text1'], \n",
    "    tfidf_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_dists = calculate_cosine_distance(tfidf_data['test']['text1'], tfidf_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf on char n-grams\n",
    "\n",
    "Very helpful if you work with russian language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))\n",
    "tfidf.fit(train['text1'].tolist() + train['text2'].tolist())\n",
    "\n",
    "tfidf_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    tfidf_data[name]['text1'] = tfidf.transform(d['text1'].tolist())\n",
    "    tfidf_data[name]['text2'] = tfidf.transform(d['text2'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data['train']['text1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    tfidf_data['dev']['text1'], \n",
    "    tfidf_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_dists = calculate_cosine_distance(tfidf_data['test']['text1'], tfidf_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Approaches -- fastText\n",
    "\n",
    "The gist of fastText is that instead of directly learning a vector representation for a word (as with word2vec), we learn a representation for each character n-gram. Each word is represented as a bag of character n-grams, so the overall word embedding is a sum of these character n-grams.\n",
    "\n",
    "fastText is a library whose purpose is to be used as a fast baseline for text embeddings/classification when deep learning approaches are just too slow and expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full corpora\n",
    "\n",
    "texts = train['text1'].tolist() + train['text2'].tolist()\n",
    "texts = [text.split() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = FastText(texts, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text, model=model):\n",
    "    assert len(text) > 0\n",
    "\n",
    "    vectors = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            vectors.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    fasttext_data[name]['text1'] = np.array([text2vec(t) for t in d['text1'].tolist()])\n",
    "    fasttext_data[name]['text2'] = np.array([text2vec(t) for t in d['text2'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data['train']['text1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    fasttext_data['dev']['text1'], \n",
    "    fasttext_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_dists = calculate_cosine_distance(fasttext_data['test']['text1'], fasttext_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained fasttext\n",
    "\n",
    "Learning word representation requires serious computational power and time. Since Facebook has done it for you, why not using that to boost productivity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you'd like to download (2.5Gb+)\n",
    "\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "# !unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format('data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    fasttext_data[name]['text1'] = np.array([text2vec(t, wv_from_text) for t in d['text1'].tolist()])\n",
    "    fasttext_data[name]['text2'] = np.array([text2vec(t, wv_from_text) for t in d['text2'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20996, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_data['train']['text1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DEV) F1 score = 0.5973247232472325\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    fasttext_data['dev']['text1'], \n",
    "    fasttext_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TEST) F1 score = 0.6051258788841006\n"
     ]
    }
   ],
   "source": [
    "test_cos_dists = calculate_cosine_distance(fasttext_data['test']['text1'], fasttext_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle texts?\n",
    "\n",
    "Embeddings!\n",
    "\n",
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "\n",
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/04/word2vec-distributed-representation.png?w=656&zoom=2\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/paraphrase'):\n",
    "    os.makedirs('data/paraphrase')\n",
    "\n",
    "train.to_csv('data/paraphrase/train.csv', index=False)\n",
    "dev.to_csv('data/paraphrase/dev.csv', index=False)\n",
    "test.to_csv('data/paraphrase/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseDataset:\n",
    "\n",
    "    def __init__(self, path, is_classification=False, min_freq=2, batch_sizes=(64, 64, 64), device=device):\n",
    "        self.path = path\n",
    "        self.is_classification = is_classification\n",
    "        self.min_freq = min_freq\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.device = device\n",
    "\n",
    "        self.text_field1 = None\n",
    "        self.text_field2 = None\n",
    "        self.labels_field = None\n",
    "        self.train_dataset, self.dev_dataset, self.test_dataset = None, None, None\n",
    "        \n",
    "        self.word2idx = None\n",
    "        self.idx2word = None\n",
    "        \n",
    "        self.build_dataset()\n",
    "        self.build_vocab()\n",
    "\n",
    "    def build_dataset(self):\n",
    "\n",
    "        self.text_field1 = torchtext.data.Field(\n",
    "            sequential=True,\n",
    "            batch_first=True,\n",
    "            lower=True,\n",
    "            preprocessing=None\n",
    "        )\n",
    "\n",
    "        self.labels_field = torchtext.data.Field(\n",
    "            sequential=False,\n",
    "            use_vocab=False,\n",
    "            is_target=True,\n",
    "            batch_first=True,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        if not self.is_classification:\n",
    "\n",
    "            self.text_field2 = torchtext.data.Field(\n",
    "                sequential=True,\n",
    "                batch_first=True,\n",
    "                lower=True,\n",
    "                preprocessing=None\n",
    "            )\n",
    "\n",
    "            fields = [\n",
    "                ('text1', self.text_field1),\n",
    "                ('text2', self.text_field2),\n",
    "                ('labels', self.labels_field)\n",
    "            ]\n",
    "        else:\n",
    "            fields = [\n",
    "                ('text1', self.text_field1),\n",
    "                ('labels', self.labels_field)\n",
    "            ]\n",
    "\n",
    "        self.train_dataset, self.dev_dataset, self.test_dataset = torchtext.data.TabularDataset.splits(\n",
    "            path=self.path,\n",
    "            root='.',\n",
    "            train='train.csv',\n",
    "            validation='dev.csv',\n",
    "            test='test.csv',\n",
    "            format='csv',\n",
    "            fields=fields,\n",
    "            skip_header=True\n",
    "        )\n",
    "\n",
    "    def build_vocab(self):\n",
    "        self.text_field1.build_vocab(self.train_dataset, min_freq=self.min_freq)\n",
    "        \n",
    "        if not self.is_classification:\n",
    "            self.text_field2.build_vocab(self.train_dataset, min_freq=self.min_freq)\n",
    "            \n",
    "            self.word2idx = defaultdict(torchtext.vocab._default_unk_index)\n",
    "            self.word2idx.update(dict(self.text_field1.vocab.stoi))\n",
    "            \n",
    "            for word, idx in self.text_field2.vocab.stoi.items():\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = len(self.word2idx)\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            self.text_field1.vocab.stoi = self.word2idx\n",
    "            self.text_field2.vocab.stoi = self.word2idx\n",
    "        else:\n",
    "            self.word2idx = dict(self.text_field1.vocab.stoi)\n",
    "        \n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        print(f'Vocabulary size = {len(self.word2idx)}')\n",
    "    \n",
    "    def create_iterators(self):\n",
    "        train_iter, dev_iter, test_iter = torchtext.data.Iterator.splits(\n",
    "            datasets=(self.train_dataset, self.dev_dataset, self.test_dataset),\n",
    "            batch_sizes=self.batch_sizes,\n",
    "            shuffle=(False, False, False),\n",
    "            sort=False,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        return train_iter, dev_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_dataset = ParaphraseDataset(path='data/paraphrase/')\n",
    "train_iter, dev_iter, test_iter = para_dataset.create_iterators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_iter))\n",
    "\n",
    "for batch in train_iter:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text1.shape, batch.text2.shape, batch.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural baseline\n",
    "\n",
    "<img src=\"https://i.ibb.co/D7R7kNH/raai-pizza.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, ntokens=len(para_dataset.word2idx), \n",
    "                 padding_idx=para_dataset.word2idx['<pad>']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.ntokens = ntokens\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=self.ntokens,\n",
    "            embedding_dim=self.emb_dim, \n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "    def forward(self, ids):\n",
    "\n",
    "        x = self.emb(ids)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = EmbeddingLayer(emb_dim=64)\n",
    "\n",
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text1.shape, batch.text2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = embedder(batch.text1)\n",
    "embeddings2 = embedder(batch.text2)\n",
    "\n",
    "embeddings1.shape, embeddings2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i0.wp.com/mlexplained.com/wp-content/uploads/2018/05/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-05-10-13.29.52.png?w=366\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingOverTime(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=self.dim)\n",
    "\n",
    "\n",
    "class AveragingNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim=64, hidden_dim=32, output_dim=16):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            MeanPoolingOverTime(),\n",
    "            nn.Linear(in_features=self.emb_dim, out_features=self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, embeds):\n",
    "\n",
    "        hidden = self.feed_forward(embeds)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = AveragingNetwork()\n",
    "\n",
    "body.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_vectors1 = body(embeddings1)\n",
    "hidden_vectors2 = body(embeddings2)\n",
    "\n",
    "hidden_vectors1.shape, hidden_vectors2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHead(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.dense = nn.Linear(in_features=self.output_dim * 2, out_features=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        concatenated = torch.cat((x, y), dim=1)\n",
    "        z = self.dense(concatenated)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = SimpleHead(output_dim=16)\n",
    "\n",
    "head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits\n",
    "output = head(hidden_vectors1, hidden_vectors2)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all together\n",
    "\n",
    "class ModelHandler(nn.Module):\n",
    "    def __init__(self, embedding_encoder, body_encoder, head_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_encoder = embedding_encoder\n",
    "        self.body_encoder = body_encoder\n",
    "        self.head_encoder = head_encoder\n",
    "\n",
    "    def forward(self, text1, text2):\n",
    "\n",
    "        hidden1 = self.predict_hidden(text1, aggregate=False)\n",
    "        hidden2 = self.predict_hidden(text2, aggregate=False)\n",
    "\n",
    "        output = self.head_encoder(hidden1, hidden2)\n",
    "\n",
    "        if len(hidden1.size()) > 2:\n",
    "            hidden1 = torch.mean(hidden1, dim=1)\n",
    "            hidden2 = torch.mean(hidden2, dim=1)\n",
    "\n",
    "        return output, (hidden1, hidden2)\n",
    "\n",
    "    def predict_hidden(self, text, aggregate=True):\n",
    "\n",
    "        embeds = self.embedding_encoder(text)\n",
    "        hidden = self.body_encoder(embeds)\n",
    "\n",
    "        if aggregate and len(hidden.size()) > 2:\n",
    "            hidden = torch.mean(hidden, dim=1)\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def predict_attention_scores(self, context, query):\n",
    "        hidden1 = self.predict_hidden(context, aggregate=False)\n",
    "        hidden2 = self.predict_hidden(query, aggregate=False)\n",
    "\n",
    "        scores = self.head_encoder.get_scores(hidden1, hidden2)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = ModelHandler(\n",
    "    embedding_encoder=embedder,\n",
    "    body_encoder=body, \n",
    "    head_encoder=head\n",
    ")\n",
    "\n",
    "baseline_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hidden1, hidden2) = baseline_model(batch.text1, batch.text2)\n",
    "\n",
    "output.shape, hidden1.shape, hidden2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*UJxVqLnbSj42eRhasKeLOA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(y_true: torch.Tensor, y_prob: torch.Tensor, \n",
    "                 thres: float = None, average: str = 'binary') -> float:\n",
    "\n",
    "    y_prob = y_prob.detach().cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "\n",
    "    if thres is None:\n",
    "        score = max([\n",
    "            f1_score(y_true, (y_prob > thres).astype(int), average=average) for thres in np.linspace(0, 1)\n",
    "        ])\n",
    "    else:\n",
    "        if average != 'binary':\n",
    "            preds = y_prob\n",
    "        else:\n",
    "            preds = (y_prob > thres).astype(int)\n",
    "        score = float(f1_score(y_true, preds, average=average))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def save_checkpoint(state_dict: dict, path: str, epoch: int) -> None:\n",
    "    torch.save(state_dict, f'{path}/model_{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "save_freq = 30\n",
    "\n",
    "\n",
    "def write_metrics(writer, step, values):\n",
    "    for name, value in values.items():\n",
    "        writer.add_scalar(name, value, global_step=step)\n",
    "\n",
    "\n",
    "def train_one_epoch(model_path, model, optimizer, iterator, writer, epoch):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator, start=(epoch - 1) * len(iterator)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, (text1_hidden, text2_hidden) = model(batch.text1, batch.text2)\n",
    "        loss = criterion(logits.squeeze(), batch.labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % save_freq == 0:\n",
    "            f1 = calculate_f1(y_true=batch.labels, y_prob=torch.sigmoid(logits))\n",
    "            write_metrics(writer, step, {'loss': loss.item(), 'f1': f1})\n",
    "            \n",
    "            print(f'[Train]  Epoch = {epoch}, Loss Value = {loss.item():.4f}, F1 score = {f1:.4f}')\n",
    "\n",
    "\n",
    "def validate(model, iterator, writer=None, epoch=None, step=None):\n",
    "    with torch.no_grad():\n",
    "        loss_history = list()\n",
    "        f1_history = list()\n",
    "        for batch in iterator:\n",
    "            logits, (text1_hidden, text2_hidden) = model(batch.text1, batch.text2)\n",
    "            loss = criterion(logits.squeeze(), batch.labels)\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            f1 = calculate_f1(y_true=batch.labels, y_prob=torch.sigmoid(logits))\n",
    "            f1_history.append(f1)\n",
    "\n",
    "        loss = np.mean(loss_history)\n",
    "        f1 = np.mean(f1_history)\n",
    "\n",
    "        if writer is not None:\n",
    "            write_metrics(writer, step, {'loss': loss, 'f1': f1})\n",
    "            print(f'>>>>>>> [Test]  Epoch = {epoch}, Loss Value = {loss:.4f}, F1 score = {f1:.4f}')\n",
    "        else:\n",
    "            return f1\n",
    "\n",
    "\n",
    "def train_evaluate(model_path,\n",
    "                   model,\n",
    "                   optimizer, \n",
    "                   train_iter,\n",
    "                   dev_iter=None,\n",
    "                   num_epochs=num_epochs, \n",
    "                   save_freq=save_freq):\n",
    "    \n",
    "    train_writer = SummaryWriter(model_path)\n",
    "    dev_writer = SummaryWriter(os.path.join(model_path, 'eval'))\n",
    "    \n",
    "    if dev_iter is not None:\n",
    "        validate(model, dev_iter, dev_writer, epoch=0, step=0)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        train_one_epoch(model_path, model, optimizer, train_iter, train_writer, epoch)\n",
    "        \n",
    "        if dev_iter is not None:\n",
    "            validate(model, dev_iter, dev_writer, epoch, step=(epoch * len(train_iter)))\n",
    "\n",
    "        # save_checkpoint(model.state_dict(), str(experiment_dir), epoch)\n",
    "        \n",
    "    train_writer.close()\n",
    "    dev_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}=-\\sum_{i=1}^{N}\\left[y_{i} \\log p_{i}+\\left(1-y_{i}\\right) \\log \\left(1-p_{i}\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.003)\n",
    "\n",
    "train_evaluate(\n",
    "    model_path='experiments/model_baseline', \n",
    "    model=baseline_model, \n",
    "    optimizer=optimizer, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex approach\n",
    "\n",
    "<img src=\"https://i.ibb.co/FXDqQbT/Screenshot-2019-07-05-at-09-51-08.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SneakyHead(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.dense = nn.Linear(in_features=self.output_dim * 4, out_features=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        emb_mul = torch.mul(x, y)\n",
    "        emb_abs = torch.abs(x - y)\n",
    "        concatenated = torch.cat([x, y, emb_abs, emb_mul], dim=1)\n",
    "        z = self.dense(concatenated)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "hidden_dim = 64\n",
    "output_dim = 32\n",
    "\n",
    "\n",
    "sneaky_model = ModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim),\n",
    "    body_encoder=AveragingNetwork(emb_dim, hidden_dim, output_dim), \n",
    "    head_encoder=SneakyHead(output_dim)\n",
    ")\n",
    "\n",
    "sneaky_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(sneaky_model.parameters(), lr=0.003)\n",
    "\n",
    "train_evaluate(\n",
    "    model_path='sneaky_model', \n",
    "    model=sneaky_model, \n",
    "    optimizer=optimizer, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try attention!\n",
    "\n",
    "But what is attention?\n",
    "\n",
    "Attention is simply a vector, often the outputs of dense layer using softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=h_{j}^{q T} W_{b} h_{t}^{p} \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\Sigma_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{b} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$\n",
    "\n",
    "### Concat Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=v_{c}^{T} \\tanh \\left(W_{c}^{1} h_{j}^{q}+W_{c}^{2} h_{t}^{p}\\right) \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\sum_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{c} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$\n",
    "\n",
    "### Dot Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=v_{d}^{T} \\tanh \\left(W_{d}\\left(h_{j}^{q} \\odot h_{t}^{p}\\right)\\right) \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\Sigma_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{d} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$\n",
    "\n",
    "### Minus Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=v_{m}^{T} \\tanh \\left(W_{m}\\left(h_{j}^{q}-h_{t}^{p}\\right)\\right) \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\Sigma_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{m} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearAttention(nn.Module):\n",
    "    # x^T W y\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores, context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        contextW = self.W(context)\n",
    "        scores = torch.bmm(contextW, query.transpose(1, 2))\n",
    "        scores = torch.softmax(scores, dim=1).transpose(2, 1)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    \n",
    "class MinusAttention(nn.Module):\n",
    "    # v^T tanh(W(x - y))\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.v = nn.Linear(self.emb_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores.transpose(2, 1), context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        batch_size, m, _ = context.size()\n",
    "        k = query.size(1)\n",
    "\n",
    "        context_ = context.repeat(1, k, 1)\n",
    "        query_ = query.repeat_interleave(m, dim=1)\n",
    "        minus = torch.sub(context_, query_)\n",
    "\n",
    "        Wminus = self.W(minus)\n",
    "        Wminus_tanh = torch.tanh(Wminus)\n",
    "\n",
    "        scores = self.v(Wminus_tanh)\n",
    "        scores = scores.reshape(batch_size, m, k)\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "class ConcatAttention(nn.Module):\n",
    "    # v^T tanh(W_1 x + W_2 y)\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W1 = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.W2 = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.v = nn.Linear(self.emb_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores.transpose(2, 1), context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        batch_size, m, _ = context.size()\n",
    "        k = query.size(1)\n",
    "\n",
    "        context_ = context.repeat(1, k, 1)\n",
    "        query_ = query.repeat_interleave(m, dim=1)\n",
    "\n",
    "        W1context = self.W1(context_)\n",
    "        W2query = self.W2(query_)\n",
    "        Wsum_tanh = torch.tanh(W1context + W2query)\n",
    "\n",
    "        scores = self.v(Wsum_tanh)\n",
    "        scores = scores.reshape(batch_size, m, k)\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "class DotAttention(nn.Module):\n",
    "    # v^T tanh(W (x * y))\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.v = nn.Linear(self.emb_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores.transpose(2, 1), context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        batch_size, m, _ = context.size()\n",
    "        k = query.size(1)\n",
    "\n",
    "        context_ = context.repeat(1, k, 1)\n",
    "        query_ = query.repeat_interleave(m, dim=1)\n",
    "        dot = torch.mul(context_, query_)\n",
    "\n",
    "        Wdot = self.W(dot)\n",
    "        Wdot_tanh = torch.tanh(Wdot)\n",
    "\n",
    "        scores = self.v(Wdot_tanh)\n",
    "        scores = scores.reshape(batch_size, m, k)\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 64\n",
    "\n",
    "att_mechanism = BilinearAttention(emb_dim=emb_dim)\n",
    "att_mechanism.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "x = torch.rand((batch_size, 3, emb_dim), device=device)\n",
    "y = torch.rand((batch_size, 5, emb_dim), device=device)\n",
    "\n",
    "# [batch_size, query_len, emb_dim]\n",
    "att_mechanism(context=x, query=y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mechanism(context=y, query=x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1838/1*8nFrwolzTYtUWSaziiJGkg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, hidden_dim, num_layers, bidirectional=False, aggregate=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.aggregate = aggregate\n",
    "        self.output_dim = self.hidden_dim * 2 if self.bidirectional else self.hidden_dim\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            self.emb_dim,\n",
    "            self.hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=self.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, embeds):\n",
    "\n",
    "        output, _ = self.rnn(embeds)\n",
    "\n",
    "        if self.aggregate:\n",
    "            output = torch.mean(output, dim=1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, output_dim, attention_mechanism):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention_mechanism = attention_mechanism(self.output_dim)\n",
    "        self.dense = nn.Linear(in_features=self.output_dim * 8, out_features=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        new_x = self.attention_mechanism(context=y, query=x)\n",
    "        new_y = self.attention_mechanism(context=x, query=y)\n",
    "        \n",
    "        x = torch.cat((new_x, x), dim=-1)\n",
    "        y = torch.cat((new_y, y), dim=-1)\n",
    "\n",
    "        x = torch.mean(x, dim=1)\n",
    "        y = torch.mean(y, dim=1)\n",
    "        \n",
    "        emb_mul = torch.mul(x, y)\n",
    "        emb_abs = torch.abs(x - y)\n",
    "        \n",
    "        concatenated = torch.cat([x, y, emb_mul, emb_abs], dim=1)\n",
    "        z = self.dense(concatenated)\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def get_scores(self, x, y):\n",
    "        scores = self.attention_mechanism.get_scores(x, y)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "\n",
    "attention_model_baseline = ModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim=emb_dim), \n",
    "    body_encoder=nn.Identity(), \n",
    "    head_encoder=AttentionHead(\n",
    "        output_dim=emb_dim, \n",
    "        attention_mechanism=BilinearAttention\n",
    "    )\n",
    ")\n",
    "\n",
    "attention_model_baseline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(attention_model_baseline.parameters(), lr=0.003)\n",
    "\n",
    "train_evaluate(\n",
    "    model_path='experiments/attention_model_baseline', \n",
    "    model=attention_model_baseline, \n",
    "    optimizer=optimizer, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "attention_model = ModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim=emb_dim), \n",
    "    body_encoder=SimpleLSTM(emb_dim=emb_dim, hidden_dim=hidden_dim, num_layers=num_layers), \n",
    "    head_encoder=AttentionHead(\n",
    "        output_dim=hidden_dim, \n",
    "        attention_mechanism=BilinearAttention\n",
    "    )\n",
    ")\n",
    "\n",
    "attention_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.003)\n",
    "\n",
    "train_evaluate(\n",
    "    model_path='experiments/attention_model', \n",
    "    model=attention_model, \n",
    "    optimizer=optimizer, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run\n",
    "\n",
    "> tensorboard --logdir experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tensor(text, dataset_handler=para_dataset):\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    \n",
    "    field = ('x', dataset_handler.train_dataset.fields['text1'])\n",
    "    examples = [torchtext.data.Example.fromlist([t], fields=[field]) for t in text]\n",
    "    dataset = torchtext.data.Dataset(examples, fields=[field])\n",
    "\n",
    "    iterator = torchtext.data.Iterator(\n",
    "        dataset=dataset,\n",
    "        batch_size=len(text),\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return next(iter(iterator)).x\n",
    "\n",
    "\n",
    "def plot_attention(query, context, att_weights, scale=True):\n",
    "    tokens_a = query.split()\n",
    "    tokens_b = context.split()\n",
    "    \n",
    "    assert len(tokens_a) == att_weights.shape[0]\n",
    "    assert len(tokens_b) == att_weights.shape[1]\n",
    "    \n",
    "    if scale:\n",
    "        mins = att_weights.min(axis=1)\n",
    "        maxes = att_weights.max(axis=1)\n",
    "        att_weights = (att_weights - mins.reshape(-1, 1))  / (maxes - mins).reshape(-1, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(att_weights, cmap='gray')\n",
    "    ax.set_xticks(np.arange(att_weights.shape[1]))\n",
    "    ax.set_yticks(np.arange(att_weights.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([word for word in tokens_b])\n",
    "    ax.set_yticklabels([word for word in tokens_a])\n",
    "\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_attention_scores(model, textA, textB):\n",
    "    with torch.no_grad():\n",
    "        attn_scores = model.predict_attention_scores(\n",
    "            text_to_tensor(textA), \n",
    "            text_to_tensor(textB)\n",
    "        ).squeeze(0)\n",
    "\n",
    "    attn_scores = attn_scores.detach().cpu().numpy()\n",
    "    \n",
    "    return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 2\n",
    "\n",
    "textA = train.iloc[ex].text1\n",
    "textB = train.iloc[ex].text2\n",
    "lab = train.iloc[ex].labels\n",
    "\n",
    "print(f'Label = {lab}')\n",
    "print(f'Text1 = {textA}')\n",
    "print(f'Text2 = {textB}')\n",
    "\n",
    "attn_scores = get_attention_scores(attention_model, textA, textB)\n",
    "\n",
    "plot_attention(textB, textA, attn_scores, scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase retriaval\n",
    "\n",
    "In reality you'll need to solve a different task. The task is given a text find its paraphrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora = pd.read_csv('data/quora_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lab, max_lab = quora['labels'].min(), quora['labels'].max()\n",
    "\n",
    "min_lab, max_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_neg_labels(lab):\n",
    "    if lab == -1:\n",
    "        lab = max_lab + 1\n",
    "    return lab\n",
    "\n",
    "quora['labels'] = quora['labels'].map(convert_neg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora['text'] = quora['text'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora.loc[quora['labels'] == 93].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, intermediate = train_test_split(quora, stratify=quora['labels'], test_size=0.3, random_state=24)\n",
    "dev, test = train_test_split(intermediate, stratify=intermediate['labels'], test_size=0.5, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/classification'):\n",
    "    os.makedirs('data/classification')\n",
    "\n",
    "train.to_csv('data/classification/train.csv', index=False)\n",
    "dev.to_csv('data/classification/dev.csv', index=False)\n",
    "test.to_csv('data/classification/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataset = ParaphraseDataset('data/classification/', is_classification=True, batch_sizes=(64, 64, 1))\n",
    "train_iter, dev_iter, test_iter = class_dataset.create_iterators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModelHandler(nn.Module):\n",
    "    def __init__(self, embedding_encoder, body_encoder, head_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_encoder = embedding_encoder\n",
    "        self.body_encoder = body_encoder\n",
    "        self.head_encoder = head_encoder\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        embeds = self.embedding_encoder(text)\n",
    "        hidden = self.body_encoder(embeds)\n",
    "        output = self.head_encoder(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "hidden_dim = 64\n",
    "output_dim = 32\n",
    "\n",
    "class_model = ClassificationModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim),\n",
    "    body_encoder=AveragingNetwork(emb_dim, hidden_dim, output_dim), \n",
    "    head_encoder=nn.Linear(in_features=output_dim, out_features=(max_lab + 2))\n",
    ")\n",
    "\n",
    "class_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = class_model(batch.text1)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to modify our functions a little\n",
    "\n",
    "def train_one_epoch_class(model_path, model, optimizer, iterator, writer, epoch):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator, start=(epoch - 1) * len(iterator)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch.text1)\n",
    "        loss = criterion(logits, batch.labels.long())\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % save_freq == 0:\n",
    "            f1 = calculate_f1(y_true=batch.labels, y_prob=torch.argmax(probs, dim=1), average='macro', thres=1)\n",
    "            write_metrics(writer, step, {'loss': loss.item(), 'f1': f1})\n",
    "            \n",
    "            print(f'[Train]  Epoch = {epoch}, Loss Value = {loss.item():.4f}, F1 score = {f1:.4f}')\n",
    "\n",
    "\n",
    "def validate_class(model, iterator, writer, epoch, step):\n",
    "    with torch.no_grad():\n",
    "        loss_history = list()\n",
    "        f1_history = list()\n",
    "        for batch in iterator:\n",
    "            logits = model(batch.text1)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            y_pred = torch.argmax(probs, dim=1)\n",
    "            f1 = calculate_f1(y_true=batch.labels, y_prob=y_pred, average='macro', thres=1)\n",
    "            f1_history.append(f1)\n",
    "    \n",
    "            loss = criterion(logits, batch.labels.long())\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "        loss = np.mean(loss_history)\n",
    "        f1 = np.mean(f1_history)\n",
    "        write_metrics(writer, step, {'loss': loss, 'f1': f1})\n",
    "        \n",
    "        print(f'>>>>>>> [Test]  Epoch = {epoch}, Loss Value = {loss:.4f}, F1 score = {f1:.4f}')\n",
    "\n",
    "\n",
    "def train_evaluate_class(model_path,\n",
    "                   model,\n",
    "                   optimizer, \n",
    "                   train_iter,\n",
    "                   dev_iter=None,\n",
    "                   num_epochs=num_epochs, \n",
    "                   save_freq=save_freq):\n",
    "    \n",
    "    train_writer = SummaryWriter(model_path)\n",
    "    dev_writer = SummaryWriter(os.path.join(model_path, 'eval'))\n",
    "    \n",
    "    if dev_iter is not None:\n",
    "        validate_class(model, dev_iter, dev_writer, epoch=0, step=0)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        train_one_epoch_class(model_path, model, optimizer, train_iter, train_writer, epoch)\n",
    "        \n",
    "        if dev_iter is not None:\n",
    "            validate_class(model, dev_iter, dev_writer, epoch, step=(epoch * len(train_iter)))\n",
    "\n",
    "        # save_checkpoint(model.state_dict(), str(experiment_dir), epoch)\n",
    "        \n",
    "    train_writer.close()\n",
    "    dev_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(class_model.parameters(), lr=0.003)\n",
    "\n",
    "train_evaluate_class(\n",
    "    model_path='experiments/classification', \n",
    "    model=class_model, \n",
    "    optimizer=optimizer, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter,\n",
    "    num_epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification dataset -> Paraphrase dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_batch in tqdm(test_iter):\n",
    "    probabilities = []\n",
    "    labels = []\n",
    "    for train_batch in train_iter:\n",
    "        mask = (train_batch.labels != max_lab + 1)\n",
    "        train_text = train_batch.text1[mask]\n",
    "        train_labels = train_batch.labels[mask]\n",
    "\n",
    "        test_text = torch.repeat_interleave(test_batch.text1, train_text.shape[0], dim=0)\n",
    "\n",
    "        logits = attention_model(test_text, train_text)[0]\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        probabilities.append(probs.detach().cpu().numpy())\n",
    "        labels.append(train_labels.cpu().numpy())\n",
    "    \n",
    "    probabilities = np.array(probabilities)\n",
    "    labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataset = ParaphraseDataset('data/classification/', is_classification=True, batch_sizes=(1, 1, 1))\n",
    "train_iter, dev_iter, test_iter = class_dataset.create_iterators()\n",
    "\n",
    "x_train_texts = []\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for train_batch in tqdm(train_iter):\n",
    "    mask = (train_batch.labels != max_lab + 1)\n",
    "    if mask.sum() > 0:\n",
    "        train_text = train_batch.text1  # [mask]\n",
    "        train_labels = train_batch.labels  # [mask]\n",
    "\n",
    "        vectors = attention_model.predict_hidden(train_text, aggregate=True)\n",
    "        x_train_texts.append(train_text.cpu().numpy())\n",
    "        x_train.append(vectors.detach().cpu().numpy())\n",
    "        y_train.append(train_labels.cpu().numpy())\n",
    "\n",
    "x_train = np.vstack(x_train)\n",
    "y_train = np.hstack(y_train)\n",
    "\n",
    "x_test_texts = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for test_batch in tqdm(test_iter):\n",
    "    test_text = test_batch.text1\n",
    "    test_labels = test_batch.labels\n",
    "    \n",
    "    vectors = attention_model.predict_hidden(test_text, aggregate=True)\n",
    "    x_test_texts.append(test_text.cpu().numpy())\n",
    "    x_test.append(vectors.detach().cpu().numpy())\n",
    "    y_test.append(test_labels.cpu().numpy())\n",
    "    \n",
    "x_test = np.vstack(x_test)\n",
    "y_test = np.hstack(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot = Approx kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "\n",
    "emb_size = x_train.shape[1]\n",
    "faiss_index = faiss.IndexFlat(emb_size)\n",
    "\n",
    "faiss_index.verbose = True\n",
    "faiss_index.add(x_train)\n",
    "\n",
    "predicted_probs = []\n",
    "predicted_labels = []\n",
    "\n",
    "for i in tqdm(range(y_test.shape[0])):\n",
    "    _, indexes = faiss_index.search(x_test[i].reshape(1, -1), k=k)\n",
    "    train_texts = [x_train_texts[q] for q in indexes[0]]\n",
    "    train_labels = y_train[indexes[0]]\n",
    "    \n",
    "    test_text = x_test_texts[i]\n",
    "    test_label = y_test[i]\n",
    "    \n",
    "    test_probs = []\n",
    "    for j in range(k):\n",
    "        train_text = train_texts[j]\n",
    "        logits, _ = attention_model(\n",
    "            torch.from_numpy(train_text).to(device),\n",
    "            torch.from_numpy(test_text).to(device)\n",
    "        )\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        test_probs.append(probs[0][0])\n",
    "        \n",
    "    test_probs = np.array(test_probs)\n",
    "    max_prob_idx = np.argmax(test_probs)\n",
    "    max_prob = test_probs[max_prob_idx]\n",
    "    pred_label = train_labels[max_prob_idx]\n",
    "    \n",
    "    predicted_probs.append(max_prob)\n",
    "    predicted_labels.append(pred_label)\n",
    "    \n",
    "predicted_probs = np.array(predicted_probs)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "best_thres = None\n",
    "best_f1 = 0\n",
    "\n",
    "for thres in np.linspace(0, 1):\n",
    "\n",
    "    y_pred = predicted_labels.copy()\n",
    "    y_pred[y_pred < thres] = max_lab + 1\n",
    "\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thres = thres\n",
    "\n",
    "print(f'F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(text1, text2):\n",
    "    vocab1 = set(text1.split())\n",
    "    vocab2 = set(text2.split())\n",
    "    int_size = len(vocab1 & vocab2)\n",
    "    un_size = len(vocab1 | vocab2)\n",
    "    if un_size > 0:\n",
    "        return int_size / un_size\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download train_new from here: https://yadi.sk/d/ProgN30MTkkEFQ\n",
    "# or uncomment code below\n",
    "\n",
    "# train_new = []\n",
    "# num_pos = 5\n",
    "# num_neg = 5\n",
    "\n",
    "# for y in tqdm(train['labels'].unique()):\n",
    "#     pos = train[train['labels'] == y]\n",
    "#     neg = train[train['labels'] != y]\n",
    "    \n",
    "#     for _ in range(num_pos):\n",
    "#         chosen_pos = np.random.choice(pos['text'].tolist(), 2, replace=False).tolist()\n",
    "#         train_new.append(chosen_pos + [1])\n",
    "        \n",
    "#     for t in np.random.permutation(pos['text'].tolist())[:num_neg]:\n",
    "#         for tn in np.random.permutation(neg['text'].tolist()):\n",
    "#             jacc = jaccard(t, tn)\n",
    "#             if (jacc > 0.01) and (jacc < 0.9):\n",
    "#                 train_new.append([t, tn, 0])\n",
    "#                 break\n",
    "        \n",
    "# train_new = pd.DataFrame(train_new, columns=['text1', 'text2', 'labels']).drop_duplicates()\n",
    "# train_new = train_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download train_new from here: https://yadi.sk/d/ProgN30MTkkEFQ\n",
    "# or uncomment code above\n",
    "\n",
    "train_new = pd.read_csv('data/new_paraphrase/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/new_paraphrase'):\n",
    "    os.makedirs('data/new_paraphrase')\n",
    "    \n",
    "train_new.to_csv('data/new_paraphrase/train.csv', index=False)\n",
    "train_new[:100].to_csv('data/new_paraphrase/dev.csv', index=False)\n",
    "train_new[:100].to_csv('data/new_paraphrase/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_dataset = ParaphraseDataset(path='data/new_paraphrase/')\n",
    "train_iter, dev_iter, test_iter = para_dataset.create_iterators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "attention_model = ModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim=emb_dim), \n",
    "    body_encoder=nn.Identity(), \n",
    "    head_encoder=AttentionHead(\n",
    "        output_dim=emb_dim, \n",
    "        attention_mechanism=BilinearAttention\n",
    "    )\n",
    ")\n",
    "\n",
    "attention_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.003)\n",
    "\n",
    "train_evaluate(\n",
    "    model_path='experiments/attention_model_new', \n",
    "    model=attention_model, \n",
    "    optimizer=optimizer, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter, \n",
    "    num_epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataset = ParaphraseDataset('data/classification/', is_classification=True, batch_sizes=(1, 1, 1))\n",
    "train_iter, dev_iter, test_iter = class_dataset.create_iterators()\n",
    "\n",
    "x_train_texts = []\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for train_batch in tqdm(train_iter):\n",
    "    mask = (train_batch.labels != max_lab + 1)\n",
    "    if mask.sum() > 0:\n",
    "        train_text = train_batch.text1  # [mask]\n",
    "        train_labels = train_batch.labels  # [mask]\n",
    "\n",
    "        vectors = attention_model.predict_hidden(train_text, aggregate=True)\n",
    "        x_train_texts.append(train_text.cpu().numpy())\n",
    "        x_train.append(vectors.detach().cpu().numpy())\n",
    "        y_train.append(train_labels.cpu().numpy())\n",
    "\n",
    "x_train = np.vstack(x_train)\n",
    "y_train = np.hstack(y_train)\n",
    "\n",
    "x_test_texts = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for test_batch in tqdm(test_iter):\n",
    "    test_text = test_batch.text1\n",
    "    test_labels = test_batch.labels\n",
    "    \n",
    "    vectors = attention_model.predict_hidden(test_text, aggregate=True)\n",
    "    x_test_texts.append(test_text.cpu().numpy())\n",
    "    x_test.append(vectors.detach().cpu().numpy())\n",
    "    y_test.append(test_labels.cpu().numpy())\n",
    "    \n",
    "x_test = np.vstack(x_test)\n",
    "y_test = np.hstack(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "\n",
    "emb_size = x_train.shape[1]\n",
    "faiss_index = faiss.IndexFlat(emb_size)\n",
    "faiss_index.verbose = True\n",
    "faiss_index.add(x_train)\n",
    "\n",
    "predicted_probs = []\n",
    "predicted_labels = []\n",
    "\n",
    "for i in tqdm(range(y_test.shape[0])):\n",
    "    _, indexes = faiss_index.search(x_test[i].reshape(1, -1), k=k)\n",
    "    train_texts = [x_train_texts[q] for q in indexes[0]]\n",
    "    train_labels = y_train[indexes[0]]\n",
    "    \n",
    "    test_text = x_test_texts[i]\n",
    "    test_label = y_test[i]\n",
    "    \n",
    "    test_probs = []\n",
    "    for j in range(k):\n",
    "        train_text = train_texts[j]\n",
    "        logits, _ = attention_model(\n",
    "            torch.from_numpy(train_text).to(device),\n",
    "            torch.from_numpy(test_text).to(device)\n",
    "        )\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        test_probs.append(probs[0][0])\n",
    "        \n",
    "    test_probs = np.array(test_probs)\n",
    "    max_prob_idx = np.argmax(test_probs)\n",
    "    max_prob = test_probs[max_prob_idx]\n",
    "    pred_label = train_labels[max_prob_idx]\n",
    "    \n",
    "    predicted_probs.append(max_prob)\n",
    "    predicted_labels.append(pred_label)\n",
    "    \n",
    "predicted_probs = np.array(predicted_probs)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "best_thres = None\n",
    "best_f1 = 0\n",
    "for thres in np.linspace(0, 1):\n",
    "\n",
    "    y_pred = predicted_labels.copy()\n",
    "    y_pred[y_pred < thres] = max_lab + 1\n",
    "\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thres = thres\n",
    "\n",
    "print(f'F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve?\n",
    "\n",
    "* include triplet loss\n",
    "* gain more data\n",
    "* complex architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "* [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "* [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/pdf/1705.02364.pdf)\n",
    "* [Multiway Attention Networks for Modeling Sentence Pairs](https://www.ijcai.org/proceedings/2018/0613.pdf)\n",
    "* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "* [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
